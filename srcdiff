diff --git a/flo-kernel/arch/arm/include/asm/unistd.h b/flo-kernel/arch/arm/include/asm/unistd.h
index 512cd14..680a00f 100644
--- a/flo-kernel/arch/arm/include/asm/unistd.h
+++ b/flo-kernel/arch/arm/include/asm/unistd.h
@@ -404,6 +404,7 @@
 #define __NR_setns			(__NR_SYSCALL_BASE+375)
 #define __NR_process_vm_readv		(__NR_SYSCALL_BASE+376)
 #define __NR_process_vm_writev		(__NR_SYSCALL_BASE+377)
+#define __NR_sched_set_CPUgroup		(__NR_SYSCALL_BASE+378)
 
 /*
  * The following SWIs are ARM private.
diff --git a/flo-kernel/arch/arm/kernel/calls.S b/flo-kernel/arch/arm/kernel/calls.S
index 463ff4a..ec93f0a 100644
--- a/flo-kernel/arch/arm/kernel/calls.S
+++ b/flo-kernel/arch/arm/kernel/calls.S
@@ -387,6 +387,7 @@
 /* 375 */	CALL(sys_setns)
 		CALL(sys_process_vm_readv)
 		CALL(sys_process_vm_writev)
+		CALL(sys_sched_set_CPUgroup)
 #ifndef syscalls_counted
 .equ syscalls_padding, ((NR_syscalls + 3) & ~3) - NR_syscalls
 #define syscalls_counted
diff --git a/flo-kernel/include/linux/init_task.h b/flo-kernel/include/linux/init_task.h
index e4baff5..4fb3f18 100644
--- a/flo-kernel/include/linux/init_task.h
+++ b/flo-kernel/include/linux/init_task.h
@@ -147,7 +147,7 @@ extern struct cred init_cred;
 	.prio		= MAX_PRIO-20,					\
 	.static_prio	= MAX_PRIO-20,					\
 	.normal_prio	= MAX_PRIO-20,					\
-	.policy		= SCHED_NORMAL,					\
+	.policy		= SCHED_GRR,					\
 	.cpus_allowed	= CPU_MASK_ALL,					\
 	.mm		= NULL,						\
 	.active_mm	= &init_mm,					\
diff --git a/flo-kernel/include/linux/interrupt.h b/flo-kernel/include/linux/interrupt.h
index 7d2b77e..52ce4b9 100644
--- a/flo-kernel/include/linux/interrupt.h
+++ b/flo-kernel/include/linux/interrupt.h
@@ -431,6 +431,7 @@ enum
 	BLOCK_IOPOLL_SOFTIRQ,
 	TASKLET_SOFTIRQ,
 	SCHED_SOFTIRQ,
+	SCHED_SOFTIRQ_GRR,
 	HRTIMER_SOFTIRQ,
 	RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */
 
diff --git a/flo-kernel/include/linux/sched.h b/flo-kernel/include/linux/sched.h
index ff6bb0f..46cf567 100644
--- a/flo-kernel/include/linux/sched.h
+++ b/flo-kernel/include/linux/sched.h
@@ -39,6 +39,7 @@
 #define SCHED_BATCH		3
 /* SCHED_ISO: reserved but not implemented yet */
 #define SCHED_IDLE		5
+#define SCHED_GRR		6
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
 
@@ -1248,11 +1249,20 @@ struct sched_rt_entity {
 #endif
 };
 
+
+struct sched_grr_entity {
+	unsigned int time_slice;
+	struct list_head list;
+};
+
 /*
  * default timeslice is 100 msecs (used only for SCHED_RR tasks).
  * Timeslices get refilled after they expire.
  */
 #define RR_TIMESLICE		(100 * HZ / 1000)
+#define GRR_TIMESLICE		(100 * HZ / 1000)
+
+#define GRR_LB_THRESH		(500 * HZ / 1000)
 
 struct rcu_node;
 
@@ -1281,6 +1291,7 @@ struct task_struct {
 	const struct sched_class *sched_class;
 	struct sched_entity se;
 	struct sched_rt_entity rt;
+	struct sched_grr_entity grr;
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	/* list of struct preempt_notifier: */
diff --git a/flo-kernel/include/linux/syscalls.h b/flo-kernel/include/linux/syscalls.h
index 3de3acb..6d67be1 100644
--- a/flo-kernel/include/linux/syscalls.h
+++ b/flo-kernel/include/linux/syscalls.h
@@ -858,4 +858,5 @@ asmlinkage long sys_process_vm_writev(pid_t pid,
 				      unsigned long riovcnt,
 				      unsigned long flags);
 
+asmlinkage long sys_sched_set_CPUgroup(int numCPU, int group);
 #endif
diff --git a/flo-kernel/kernel/cpu.c b/flo-kernel/kernel/cpu.c
index 4f9701b..cdee128 100644
--- a/flo-kernel/kernel/cpu.c
+++ b/flo-kernel/kernel/cpu.c
@@ -409,7 +409,7 @@ int disable_nonboot_cpus(void)
 	for_each_online_cpu(cpu) {
 		if (cpu == first_cpu)
 			continue;
-		error = 0;//_cpu_down(cpu, 1);
+		error = 1;
 		if (!error)
 			cpumask_set_cpu(cpu, frozen_cpus);
 		else {
diff --git a/flo-kernel/kernel/kthread.c b/flo-kernel/kernel/kthread.c
index 3d3de63..3de2112 100644
--- a/flo-kernel/kernel/kthread.c
+++ b/flo-kernel/kernel/kthread.c
@@ -203,7 +203,7 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 		 * root may have changed our (kthreadd's) priority or CPU mask.
 		 * The kernel thread should not inherit these properties.
 		 */
-		sched_setscheduler_nocheck(create.result, SCHED_NORMAL, &param);
+		sched_setscheduler_nocheck(create.result, SCHED_GRR, &param);
 		set_cpus_allowed_ptr(create.result, cpu_all_mask);
 	}
 	return create.result;
diff --git a/flo-kernel/kernel/sched/core.c b/flo-kernel/kernel/sched/core.c
index 1cee48f..4371cff 100644
--- a/flo-kernel/kernel/sched/core.c
+++ b/flo-kernel/kernel/sched/core.c
@@ -1768,7 +1768,8 @@ void sched_fork(struct task_struct *p)
 	 */
 	if (unlikely(p->sched_reset_on_fork)) {
 		if (task_has_rt_policy(p)) {
-			p->policy = SCHED_NORMAL;
+			p->policy = SCHED_GRR;
+			/* p->policy = SCHED_NORMAL; */
 			p->static_prio = NICE_TO_PRIO(0);
 			p->rt_priority = 0;
 		} else if (PRIO_TO_NICE(p->static_prio) < 0)
@@ -1785,7 +1786,8 @@ void sched_fork(struct task_struct *p)
 	}
 
 	if (!rt_prio(p->prio))
-		p->sched_class = &fair_sched_class;
+		p->sched_class = &sched_grr_class;
+		/* p->sched_class = &fair_sched_class; */
 
 	if (p->sched_class->task_fork)
 		p->sched_class->task_fork(p);
@@ -3051,6 +3053,7 @@ void scheduler_tick(void)
 #ifdef CONFIG_SMP
 	rq->idle_balance = idle_cpu(cpu);
 	trigger_load_balance(rq, cpu);
+	trigger_load_balance_grr(rq, cpu);
 #endif
 }
 
@@ -4061,6 +4064,8 @@ __setscheduler(struct rq *rq, struct task_struct *p, int policy, int prio)
 	p->prio = rt_mutex_getprio(p);
 	if (rt_prio(p->prio))
 		p->sched_class = &rt_sched_class;
+	else if (p->policy == SCHED_GRR)
+		p->sched_class = &sched_grr_class;
 	else
 		p->sched_class = &fair_sched_class;
 	set_load_weight(p);
@@ -4107,7 +4112,7 @@ recheck:
 
 		if (policy != SCHED_FIFO && policy != SCHED_RR &&
 				policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-				policy != SCHED_IDLE)
+				policy != SCHED_IDLE && policy != SCHED_GRR)
 			return -EINVAL;
 	}
 
@@ -4310,6 +4315,37 @@ SYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy,
 	return do_sched_setscheduler(pid, policy, param);
 }
 
+static DEFINE_SPINLOCK(assign_cpus_lock);
+
+SYSCALL_DEFINE2(sched_set_CPUgroup, int, numCPU, int, group)
+{
+	int num_fg_1, ret;
+
+	if (get_current_user()->uid != 0)
+		return -EACCES;
+
+	if (numCPU < 1 || numCPU > (nr_cpu_ids - 1))
+		return -EINVAL;
+
+	if (group != FOREGROUND && group != BACKGROUND)
+		return -EINVAL;
+
+	if (group == FOREGROUND)
+		num_fg_1 = numCPU;
+	else
+		num_fg_1 = nr_cpu_ids - numCPU;
+
+
+	spin_lock(&assign_cpus_lock);
+
+	ret = assign_groups_grr(num_fg_1);
+
+	spin_unlock(&assign_cpus_lock);
+
+	return 0;
+}
+
+
 /**
  * sys_sched_setparam - set/change the RT priority of a thread
  * @pid: the pid in question.
@@ -4785,6 +4821,7 @@ SYSCALL_DEFINE1(sched_get_priority_max, int, policy)
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
 	case SCHED_IDLE:
+	case SCHED_GRR:
 		ret = 0;
 		break;
 	}
@@ -4810,6 +4847,7 @@ SYSCALL_DEFINE1(sched_get_priority_min, int, policy)
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
 	case SCHED_IDLE:
+	case SCHED_GRR:
 		ret = 0;
 	}
 	return ret;
@@ -7003,6 +7041,8 @@ void __init sched_init(void)
 		rq->calc_load_update = jiffies + LOAD_FREQ;
 		init_cfs_rq(&rq->cfs);
 		init_rt_rq(&rq->rt, rq);
+		/* Initialize GRR */
+		init_grr_rq(&rq->grr, rq, i);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 		root_task_group.shares = ROOT_TASK_GROUP_LOAD;
 		INIT_LIST_HEAD(&rq->leaf_cfs_rq_list);
@@ -7093,7 +7133,8 @@ void __init sched_init(void)
 	/*
 	 * During early bootup we pretend to be a normal task:
 	 */
-	current->sched_class = &fair_sched_class;
+	/* current->sched_class = &fair_sched_class; */
+	current->sched_class = &sched_grr_class;
 
 #ifdef CONFIG_SMP
 	zalloc_cpumask_var(&sched_domains_tmpmask, GFP_NOWAIT);
@@ -7163,7 +7204,8 @@ static void normalize_task(struct rq *rq, struct task_struct *p)
 	on_rq = p->on_rq;
 	if (on_rq)
 		dequeue_task(rq, p, 0);
-	__setscheduler(rq, p, SCHED_NORMAL, 0);
+	/* __setscheduler(rq, p, SCHED_NORMAL, 0); */
+	__setscheduler(rq, p, SCHED_GRR, 0);
 	if (on_rq) {
 		enqueue_task(rq, p, 0);
 		resched_task(rq->curr);
@@ -7346,6 +7388,11 @@ void sched_move_task(struct task_struct *tsk)
 	unsigned long flags;
 	struct rq *rq;
 
+#ifdef CONFIG_SMP
+	int cpu;
+	struct rq *new_rq;
+#endif
+
 	rq = task_rq_lock(tsk, &flags);
 
 	running = task_current(rq, tsk);
@@ -7369,6 +7416,34 @@ void sched_move_task(struct task_struct *tsk)
 		enqueue_task(rq, tsk, 0);
 
 	task_rq_unlock(rq, tsk, &flags);
+
+#ifdef CONFIG_SMP
+
+	if (tsk->policy == SCHED_GRR) {
+		cpu = tsk->sched_class->select_task_rq(tsk, 0, 0);
+		new_rq = cpu_rq(cpu);
+
+		if (new_rq == rq)
+			return;
+
+		if (running)
+			return;
+
+		if (!on_rq)
+			return;
+
+		raw_spin_lock_irq(&rq->lock);
+		double_lock_balance(rq, new_rq);
+
+		deactivate_task(rq, tsk, 0);
+		set_task_cpu(tsk, cpu);
+		activate_task(new_rq, tsk, 0);
+		check_preempt_curr(new_rq, tsk, 0);
+
+		double_unlock_balance(rq, new_rq);
+		raw_spin_unlock_irq(&rq->lock);
+	}
+#endif
 }
 #endif /* CONFIG_CGROUP_SCHED */
 
@@ -7724,8 +7799,9 @@ static void cpu_cgroup_attach(struct cgroup *cgrp,
 {
 	struct task_struct *task;
 
-	cgroup_taskset_for_each(task, cgrp, tset)
+	cgroup_taskset_for_each(task, cgrp, tset) {
 		sched_move_task(task);
+	}
 }
 
 static void
diff --git a/flo-kernel/kernel/sched/grr.c b/flo-kernel/kernel/sched/grr.c
new file mode 100644
index 0000000..30bad6f
--- /dev/null
+++ b/flo-kernel/kernel/sched/grr.c
@@ -0,0 +1,566 @@
+#include "sched.h"
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/string.h>
+
+#ifdef CONFIG_SMP
+
+static struct sched_grr_entity
+*get_next_elegible_entity(struct rq *rq, int dst_cpu);
+
+#endif
+
+#ifdef CONFIG_CGROUP_SCHED
+#define PATH_MAX 4096
+
+static char group_path[PATH_MAX];
+
+static char *task_group_path(struct task_group *tg)
+{
+	if (autogroup_path(tg, group_path, PATH_MAX))
+		return group_path;
+	/*
+	 * May be NULL if the underlying cgroup isn't fully-created yet
+	 */
+	if (!tg->css.cgroup) {
+		group_path[0] = '\0';
+		return group_path;
+	}
+	cgroup_path(tg->css.cgroup, group_path, PATH_MAX);
+	return group_path;
+}
+
+
+static int get_task_group(struct task_struct *p)
+{
+	char *path;
+	int ret = 0;
+
+	path = task_group_path(task_group(p));
+
+	if (strcmp(path, "/") == 0 || strcmp(path, "/apps") == 0)
+		ret = 1;
+	else if (strcmp(path, "/apps/bg_non_interactive") == 0)
+		ret = 2;
+
+	return ret;
+}
+
+
+int get_task_group_grr(struct task_struct *p)
+{
+	return get_task_group(p);
+}
+#endif
+
+
+/*
+ * Grouper Round-Robin scheduling class.
+ */
+
+#ifdef CONFIG_SMP
+
+void trigger_load_balance_grr(struct rq *rq, int cpu)
+{
+	struct grr_rq *grr_rq = &rq->grr;
+
+	grr_rq->tick_count++;
+
+	if (grr_rq->tick_count >= grr_rq->load_balance_thresh) {
+		grr_rq->load_balance_thresh += GRR_LB_THRESH;
+		raise_softirq(SCHED_SOFTIRQ_GRR);
+	}
+}
+
+static int
+find_min_rq_cpu(struct task_struct *p)
+{
+	int cpu, min_cpu = 0, min_running = 0, first = 1, group;
+	struct rq *rq;
+	struct grr_rq *grr_rq;
+
+	group = get_task_group(p);
+
+	for_each_possible_cpu(cpu) {
+		rq = cpu_rq(cpu);
+
+		grr_rq = &rq->grr;
+
+		if (grr_rq->group != group)
+			continue;
+
+		if (first) {
+			min_running = rq->grr.nr_running;
+			min_cpu = cpu;
+			first = 0;
+			continue;
+		}
+		if (min_running > rq->grr.nr_running) {
+			min_running = rq->grr.nr_running;
+			min_cpu = cpu;
+		}
+	}
+	return min_cpu;
+}
+
+static int
+select_task_rq_grr(struct task_struct *p, int sd_flag, int flags)
+{
+	int min_cpu = 0;
+	struct rq *rq;
+
+	min_cpu = find_min_rq_cpu(p);
+	rq = cpu_rq(min_cpu);
+	/*trace_printk("Selected CPU: %d\tNR: %d\n", min_cpu,
+rq->grr.nr_running);*/
+	return min_cpu;
+}
+#endif /* CONFIG_SMP */
+/*
+ * Idle tasks are unconditionally rescheduled:
+ */
+static void check_preempt_curr_grr(struct rq *rq,
+		struct task_struct *p, int flags)
+{
+	/*printk(KERN_WARNING "NICOLAS: Check preempt curr\n");*/
+}
+
+static struct task_struct *pick_next_task_grr(struct rq *rq)
+{
+	struct grr_rq *grr_rq = &rq->grr;
+	struct  sched_grr_entity *entity;
+	struct task_struct *p;
+
+	/* No tasks in queue */
+	if (list_empty(&grr_rq->queue))
+		return NULL;
+
+	entity = list_first_entry(&grr_rq->queue,
+		struct sched_grr_entity, list);
+
+	p = container_of(entity, struct task_struct, grr);
+	return p;
+}
+
+static void
+dequeue_task_grr(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct grr_rq *grr_rq = &rq->grr;
+	struct sched_grr_entity *entity = &p->grr;
+
+	if (grr_rq->nr_running) {
+		grr_rq->nr_running--;
+		list_del(&entity->list);
+	}
+}
+
+static void yield_task_grr(struct rq *rq)
+{
+	/*printk(KERN_WARNING "NICOLAS: Called yield task\n");*/
+}
+
+static void
+enqueue_task_grr(struct rq *rq, struct task_struct *p, int flags)
+{
+	struct grr_rq *grr_rq = &rq->grr;
+	struct sched_grr_entity *entity = &p->grr;
+
+	grr_rq->nr_running++;
+	list_add(&entity->list, &grr_rq->queue);
+}
+
+static void put_prev_task_grr(struct rq *rq, struct task_struct *p)
+{
+}
+
+static void task_tick_grr(struct rq *rq, struct task_struct *curr, int queued)
+{
+	struct sched_grr_entity *entity = &curr->grr;
+
+	if ((--(entity->time_slice)) > 0)
+		return;
+
+	entity->time_slice = GRR_TIMESLICE;
+
+	if (rq->grr.nr_running > 1) {
+		list_move(&rq->grr.queue, &entity->list);
+		set_tsk_need_resched(curr);
+	}
+}
+
+static void set_curr_task_grr(struct rq *rq)
+{
+	struct task_struct *p = rq->curr;
+
+	p->se.exec_start = rq->clock_task;
+	p->grr.time_slice = GRR_TIMESLICE;
+}
+
+static void switched_to_grr(struct rq *rq, struct task_struct *p)
+{
+}
+
+static void
+prio_changed_grr(struct rq *rq, struct task_struct *p, int oldprio)
+{
+}
+
+static unsigned int get_rr_interval_grr(struct rq *rq,
+	struct task_struct *task)
+{
+	return 0;
+}
+
+#ifdef CONFIG_SMP
+
+static struct sched_grr_entity *get_next_elegible_entity(struct rq *rq,
+	int dst_cpu)
+{
+	struct grr_rq *grr_rq = &rq->grr;
+	int num = 1;
+	struct sched_grr_entity *entity;
+	struct task_struct *p;
+
+	list_for_each_entry(entity, &grr_rq->queue, list) {
+		if (num >= 2) {
+			p = container_of(entity, struct task_struct, grr);
+			if (cpumask_test_cpu(dst_cpu, tsk_cpus_allowed(p))
+				&& !task_running(rq, p))
+				return entity;
+		}
+		num++;
+	}
+	return NULL;
+}
+
+static struct sched_grr_entity *get_next_elegible_entity_grp(
+	struct rq *rq, int dst_cpu, int group)
+{
+	struct grr_rq *grr_rq = &rq->grr;
+	int num = 1;
+	struct sched_grr_entity *entity;
+	struct task_struct *p;
+
+	list_for_each_entry(entity, &grr_rq->queue, list) {
+		if (num >= 2) {
+			p = container_of(entity, struct task_struct, grr);
+			if (cpumask_test_cpu(dst_cpu, tsk_cpus_allowed(p))
+				&& !task_running(rq, p) && get_task_group(p)
+					== group)
+					return entity;
+		}
+		num++;
+	}
+	return NULL;
+}
+
+
+static void pre_schedule_grr(struct rq *rq, struct task_struct *prev)
+{
+		int cpu;
+		struct rq *src_rq;
+		struct sched_grr_entity *entity;
+		struct grr_rq *grr_rq = &rq->grr;
+		struct task_struct *p;
+
+		if (rq->grr.nr_running == 0) {
+			for_each_possible_cpu(cpu) {
+				src_rq = cpu_rq(cpu);
+
+				if (src_rq == rq)
+					continue;
+
+				 if (src_rq->grr.group != grr_rq->group)
+					continue;
+
+				if (src_rq->grr.nr_running < 2)
+					continue;
+
+				double_lock_balance(rq, src_rq);
+
+				if (src_rq->grr.nr_running < 2) {
+					double_unlock_balance(rq, src_rq);
+					continue;
+				}
+
+				entity = get_next_elegible_entity(
+					src_rq, cpu_of(rq));
+
+				if (entity != NULL) {
+					p = container_of(entity,
+						struct task_struct, grr);
+					deactivate_task(src_rq, p, 0);
+					set_task_cpu(p, cpu_of(rq));
+					activate_task(rq, p, 0);
+				}
+
+				double_unlock_balance(rq, src_rq);
+			}
+		}
+}
+
+static void run_rebalance_domains_grr(struct softirq_action *h)
+{
+	int cpu, min_running_g1 = 0, min_running_g2 = 0,
+			 max_running_g1 = 0, max_running_g2 = 0,
+			 first_g1 = 1, first_g2 = 1;
+
+	struct rq *min_rq_g1 = NULL;
+	struct rq *min_rq_g2 = NULL;
+	struct rq *max_rq_g1 = NULL;
+	struct rq *max_rq_g2 = NULL;
+	struct grr_rq *grr_rq = NULL;
+	struct sched_grr_entity *entity = NULL;
+	struct task_struct *p = NULL;
+	struct rq *rq = NULL;
+
+	for_each_possible_cpu(cpu) {
+		rq = cpu_rq(cpu);
+		raw_spin_lock(&rq->lock);
+		grr_rq = &rq->grr;
+		if (grr_rq->group == 1) {
+			if (first_g1) {
+				min_rq_g1 = max_rq_g1 = rq;
+				min_running_g1 = max_running_g1 =
+					grr_rq->nr_running;
+				first_g1 = 0;
+				raw_spin_unlock(&rq->lock);
+				continue;
+			}
+
+			if (grr_rq->nr_running < min_running_g1) {
+				min_running_g1 = grr_rq->nr_running;
+				min_rq_g1 = rq;
+			}
+
+			if (grr_rq->nr_running > max_running_g1) {
+				max_running_g1 = grr_rq->nr_running;
+				max_rq_g1 = rq;
+			}
+		} else {
+			if (first_g2) {
+				min_rq_g2 = max_rq_g2 = rq;
+				min_running_g2 = max_running_g2 =
+					grr_rq->nr_running;
+				first_g2 = 0;
+
+				raw_spin_unlock(&rq->lock);
+				continue;
+			}
+
+			if (grr_rq->nr_running < min_running_g2) {
+				min_running_g2 = grr_rq->nr_running;
+				min_rq_g2 = rq;
+			}
+
+			if (grr_rq->nr_running > max_running_g2) {
+				max_running_g2 = grr_rq->nr_running;
+				max_rq_g2 = rq;
+			}
+		}
+
+		raw_spin_unlock(&rq->lock);
+	}
+
+	if (max_running_g1 - min_running_g1 >= 2) {
+		raw_spin_lock_irq(&max_rq_g1->lock);
+		double_lock_balance(max_rq_g1, min_rq_g1);
+
+		min_running_g1 = min_rq_g1->grr.nr_running;
+		max_running_g1 = max_rq_g1->grr.nr_running;
+
+		/* Maybe this changed while grabbing the locks */
+		if (max_running_g1 - min_running_g1 < 2) {
+			double_unlock_balance(max_rq_g1, min_rq_g1);
+			raw_spin_unlock_irq(&max_rq_g1->lock);
+		} else {
+			grr_rq = &max_rq_g1->grr;
+
+			entity = get_next_elegible_entity(max_rq_g1,
+				cpu_of(min_rq_g1));
+
+			if (entity != NULL) {
+				p = container_of(entity,
+					struct task_struct, grr);
+
+				deactivate_task(max_rq_g1, p, 0);
+				set_task_cpu(p, cpu_of(min_rq_g1));
+				activate_task(min_rq_g1, p, 0);
+				check_preempt_curr(min_rq_g1, p, 0);
+			}
+
+			double_unlock_balance(max_rq_g1, min_rq_g1);
+			raw_spin_unlock_irq(&max_rq_g1->lock);
+		}
+	}
+
+	if (max_running_g2 - min_running_g2 >= 2) {
+		raw_spin_lock_irq(&max_rq_g2->lock);
+		double_lock_balance(max_rq_g2, min_rq_g2);
+
+		min_running_g2 = min_rq_g2->grr.nr_running;
+		max_running_g2 = max_rq_g2->grr.nr_running;
+
+		/* Maybe this changed while grabbing the locks */
+		if (max_running_g2 - min_running_g2 < 2) {
+			double_unlock_balance(max_rq_g2, min_rq_g2);
+			raw_spin_unlock_irq(&max_rq_g2->lock);
+			return;
+		}
+
+		grr_rq = &max_rq_g2->grr;
+
+		entity = get_next_elegible_entity(max_rq_g2,
+			cpu_of(min_rq_g2));
+
+		if (entity != NULL) {
+			p = container_of(entity, struct task_struct, grr);
+
+			deactivate_task(max_rq_g2, p, 0);
+			set_task_cpu(p, cpu_of(min_rq_g2));
+			activate_task(min_rq_g2, p, 0);
+			check_preempt_curr(min_rq_g2, p, 0);
+		}
+		double_unlock_balance(max_rq_g2, min_rq_g2);
+		raw_spin_unlock_irq(&max_rq_g2->lock);
+	}
+}
+
+
+int assign_groups_grr(int num_fg_1)
+{
+	int cpu, count = 0, fg_count = 0, bg_count = 0;
+	struct rq *rq, *dst_rq;
+	struct sched_grr_entity *entity;
+	struct task_struct *p;
+
+	for_each_possible_cpu(cpu) {
+		rq = cpu_rq(cpu);
+
+		if (rq->grr.group == FOREGROUND)
+			fg_count++;
+		else
+			bg_count++;
+	}
+
+	/* Nothing to do */
+	if (num_fg_1 == fg_count)
+		return 0;
+
+	for_each_possible_cpu(cpu) {
+		rq = cpu_rq(cpu);
+
+		if (count++ < num_fg_1) {
+			if (rq->grr.group == FOREGROUND)
+				continue;
+
+			dst_rq = cpu_rq(nr_cpu_ids - 1);
+
+			raw_spin_lock_irq(&rq->lock);
+			double_lock_balance(rq, dst_rq);
+			rq->grr.group = FOREGROUND;
+
+			entity = get_next_elegible_entity_grp(rq,
+				cpu_of(dst_rq), BACKGROUND);
+			while (entity != NULL) {
+				p = container_of(entity,
+					struct task_struct, grr);
+
+				deactivate_task(rq, p, 0);
+				set_task_cpu(p, cpu_of(dst_rq));
+				activate_task(dst_rq, p, 0);
+				check_preempt_curr(dst_rq, p, 0);
+
+				entity = get_next_elegible_entity_grp(
+					rq, cpu_of(dst_rq), BACKGROUND);
+			}
+
+			double_unlock_balance(rq, dst_rq);
+			raw_spin_unlock_irq(&rq->lock);
+		} else {
+			if (rq->grr.group == BACKGROUND)
+				continue;
+
+			dst_rq = cpu_rq(0);
+
+			raw_spin_lock_irq(&rq->lock);
+			double_lock_balance(rq, dst_rq);
+
+			rq->grr.group = BACKGROUND;
+
+			entity = get_next_elegible_entity_grp(rq,
+				cpu_of(dst_rq), FOREGROUND);
+
+			while (entity != NULL) {
+				p = container_of(entity,
+					struct task_struct, grr);
+
+				deactivate_task(rq, p, 0);
+				set_task_cpu(p, cpu_of(dst_rq));
+				activate_task(dst_rq, p, 0);
+				check_preempt_curr(dst_rq, p, 0);
+
+				entity = get_next_elegible_entity_grp(
+					rq, cpu_of(dst_rq), FOREGROUND);
+			}
+
+			double_unlock_balance(rq, dst_rq);
+			raw_spin_unlock_irq(&rq->lock);
+		}
+	}
+	return 0;
+}
+
+#endif
+
+void init_grr_rq(struct grr_rq *grr_rq, struct rq *rq, int cpu)
+{
+#ifdef CONFIG_SMP
+	if (cpu < nr_cpu_ids / 2)
+		grr_rq->group = FOREGROUND;
+	else
+		grr_rq->group = BACKGROUND;
+
+	open_softirq(SCHED_SOFTIRQ_GRR, run_rebalance_domains_grr);
+#endif
+
+	grr_rq->nr_running = 0;
+
+	grr_rq->tick_count = 0;
+	grr_rq->load_balance_thresh = GRR_LB_THRESH;
+
+	INIT_LIST_HEAD(&grr_rq->queue);
+}
+
+/*
+ * Class for Grouped Round Robin Scheduler
+ */
+const struct sched_class sched_grr_class = {
+	.next = &fair_sched_class,
+
+	.enqueue_task		= enqueue_task_grr,
+
+	.dequeue_task		= dequeue_task_grr,
+
+	.yield_task		= yield_task_grr,
+
+	.check_preempt_curr	= check_preempt_curr_grr,
+
+	.pick_next_task		= pick_next_task_grr,
+	.put_prev_task		= put_prev_task_grr,
+
+#ifdef CONFIG_SMP
+	.select_task_rq		= select_task_rq_grr,
+#endif
+
+	.set_curr_task          = set_curr_task_grr,
+	.task_tick		= task_tick_grr,
+
+	.get_rr_interval	= get_rr_interval_grr,
+
+	.prio_changed		= prio_changed_grr,
+	.switched_to		= switched_to_grr,
+#ifdef CONFIG_SMP
+	.pre_schedule		= pre_schedule_grr,
+#endif
+};
diff --git a/flo-kernel/kernel/sched/rt.c b/flo-kernel/kernel/sched/rt.c
index 8f32475..dfbea87 100644
--- a/flo-kernel/kernel/sched/rt.c
+++ b/flo-kernel/kernel/sched/rt.c
@@ -2036,7 +2036,7 @@ static unsigned int get_rr_interval_rt(struct rq *rq, struct task_struct *task)
 }
 
 const struct sched_class rt_sched_class = {
-	.next			= &fair_sched_class,
+	.next			= &sched_grr_class,
 	.enqueue_task		= enqueue_task_rt,
 	.dequeue_task		= dequeue_task_rt,
 	.yield_task		= yield_task_rt,
diff --git a/flo-kernel/kernel/sched/sched.h b/flo-kernel/kernel/sched/sched.h
index 5370bcb..a9d3271 100644
--- a/flo-kernel/kernel/sched/sched.h
+++ b/flo-kernel/kernel/sched/sched.h
@@ -16,6 +16,8 @@ extern __read_mostly int scheduler_running;
 #define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
 #define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
 #define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+#define FOREGROUND 1
+#define BACKGROUND 2
 
 /*
  * 'User priority' is the nice value converted to something we
@@ -122,6 +124,7 @@ struct task_group {
 
 	struct rt_bandwidth rt_bandwidth;
 #endif
+	struct grr_rq **grr_rq;
 
 	struct rcu_head rcu;
 	struct list_head list;
@@ -176,7 +179,8 @@ static inline int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)
 extern int tg_nop(struct task_group *tg, void *data);
 
 extern void free_fair_sched_group(struct task_group *tg);
-extern int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent);
+extern int alloc_fair_sched_group(struct task_group *tg,
+	struct task_group *parent);
 extern void unregister_fair_sched_group(struct task_group *tg, int cpu);
 extern void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,
 			struct sched_entity *se, int cpu,
@@ -189,7 +193,8 @@ extern void __start_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
 extern void unthrottle_cfs_rq(struct cfs_rq *cfs_rq);
 
 extern void free_rt_sched_group(struct task_group *tg);
-extern int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent);
+extern int alloc_rt_sched_group(struct task_group *tg,
+	struct task_group *parent);
 extern void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,
 		struct sched_rt_entity *rt_se, int cpu,
 		struct sched_rt_entity *parent);
@@ -311,6 +316,14 @@ struct rt_rq {
 #endif
 };
 
+struct grr_rq {
+	int nr_running;
+	int group;
+	unsigned long tick_count;
+	unsigned long load_balance_thresh;
+	struct list_head queue;
+};
+
 #ifdef CONFIG_SMP
 
 /*
@@ -340,6 +353,7 @@ extern struct root_domain def_root_domain;
 
 #endif /* CONFIG_SMP */
 
+
 /*
  * This is the main, per-CPU runqueue data structure.
  *
@@ -372,6 +386,7 @@ struct rq {
 
 	struct cfs_rq cfs;
 	struct rt_rq rt;
+	struct grr_rq grr;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
@@ -851,16 +866,19 @@ enum cpuacct_stat_index {
 
 #define sched_class_highest (&stop_sched_class)
 #define for_each_class(class) \
-   for (class = sched_class_highest; class; class = class->next)
+for (class = sched_class_highest; class; class = class->next)
 
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
+extern const struct sched_class sched_grr_class;
 
 
 #ifdef CONFIG_SMP
 
+extern void trigger_load_balance_grr(struct rq *rq, int cpu);
+extern int get_task_group_grr(struct task_struct *p);
 extern void trigger_load_balance(struct rq *rq, int cpu);
 extern void idle_balance(int this_cpu, struct rq *this_rq);
 
@@ -876,7 +894,8 @@ extern void sysrq_sched_debug_show(void);
 extern void sched_init_granularity(void);
 extern void update_max_interval(void);
 extern void update_group_power(struct sched_domain *sd, int cpu);
-extern int update_runtime(struct notifier_block *nfb, unsigned long action, void *hcpu);
+extern int update_runtime(struct notifier_block *nfb,
+	unsigned long action, void *hcpu);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
 
@@ -884,7 +903,8 @@ extern void resched_task(struct task_struct *p);
 extern void resched_cpu(int cpu);
 
 extern struct rt_bandwidth def_rt_bandwidth;
-extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
+extern void init_rt_bandwidth(struct rt_bandwidth *rt_b,
+	u64 period, u64 runtime);
 
 extern void update_cpu_load(struct rq *this_rq);
 
@@ -1155,6 +1175,8 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+extern void init_grr_rq(struct grr_rq *grr_rq, struct rq *rq, int cpu);
+extern int assign_groups_grr(int num_fg_1);
 extern void unthrottle_offline_cfs_rqs(struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
diff --git a/test/set_sched.c b/test/set_sched.c
new file mode 100644
index 0000000..57c9fd7
--- /dev/null
+++ b/test/set_sched.c
@@ -0,0 +1,26 @@
+#include <sched.h>
+#include <string.h>
+#include <errno.h>
+#include <stdio.h>
+
+int main(int argc, char **argv)
+{
+	struct sched_param param;
+	int ret;
+
+	param.sched_priority = 0;
+
+	ret = sched_setscheduler(0, 6, &param);
+
+	if (ret == -1) {
+		printf("Error: %s\n", strerror(errno));
+		return 1;
+	}
+
+	printf("Returned: %d\n", ret);
+
+	while (1)
+		;
+
+	return 0;
+}
diff --git a/test/set_sched2.c b/test/set_sched2.c
new file mode 100644
index 0000000..b03bf20
--- /dev/null
+++ b/test/set_sched2.c
@@ -0,0 +1,13 @@
+#include <sched.h>
+#include <string.h>
+#include <errno.h>
+#include <stdio.h>
+
+int main(int argc, char **argv)
+{
+
+	while (1)
+		;
+
+	return 0;
+}
diff --git a/test/sleep_set_sched.c b/test/sleep_set_sched.c
new file mode 100644
index 0000000..1cf7d75
--- /dev/null
+++ b/test/sleep_set_sched.c
@@ -0,0 +1,28 @@
+#include <sched.h>
+#include <string.h>
+#include <errno.h>
+#include <stdio.h>
+
+int main(int argc, char **argv)
+{
+	struct sched_param param;
+	int ret;
+
+	param.sched_priority = 0;
+
+	ret = sched_setscheduler(0, 6, &param);
+
+	if (ret == -1) {
+		printf("Error: %s\n", strerror(errno));
+		return 1;
+	}
+
+	printf("Returned: %d\n", ret);
+
+	while (1) {
+		printf("Printed %d\n", ret++);
+		sleep(2);
+	}
+
+	return 0;
+}
diff --git a/test/syscall.c b/test/syscall.c
new file mode 100644
index 0000000..a57c0fd
--- /dev/null
+++ b/test/syscall.c
@@ -0,0 +1,25 @@
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <errno.h>
+#include <unistd.h>
+#include <sys/syscall.h>
+
+
+int sched_set_CPUgroup(int numCPU, int group)
+{
+	return syscall(378, numCPU, group);
+}
+
+int main(int argc, char **argv)
+{
+	int numCPU, group;
+
+	numCPU = atoi(argv[1]);
+	group = atoi(argv[2]);
+
+	if (sched_set_CPUgroup(numCPU, group))
+		printf("Error: %s\n", strerror(errno));
+
+	return 0;
+}
diff --git a/test/test.c b/test/test.c
index 7c5d50a..5019e79 100644
--- a/test/test.c
+++ b/test/test.c
@@ -1,4 +1,4 @@
-int main(int argc, char **argv) 
+int main(int argc, char **argv)
 {
 	return 0;
 }
